{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bfecd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "from hmpai.training import split_participants, split_participants_custom\n",
    "from hmpai.pytorch.training import train_and_test\n",
    "from hmpai.pytorch.utilities import DEVICE, set_global_seed, load_model\n",
    "from hmpai.pytorch.generators import MultiXArrayProbaDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from hmpai.pytorch.normalization import *\n",
    "from torchvision.transforms import Compose\n",
    "from hmpai.pytorch.transforms import *\n",
    "from hmpai.pytorch.mamba import *\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "DATA_PATH = Path(os.getenv(\"DATA_PATH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8df765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in models and data (val set)\n",
    "\n",
    "# Predict on val set, saving embeddings and predicted probas\n",
    "\n",
    "# Maybe as xr?\n",
    "# Include dataset as variable (prp/task1, prp/task2, sat_weindel, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db6c681b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'set_global_seed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mset_global_seed\u001b[49m(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      3\u001b[0m data_paths \u001b[38;5;241m=\u001b[39m [DATA_PATH \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprp/stage_data_250hz_t1.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m, DATA_PATH \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprp/stage_data_250hz_t2.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 80/20 train/val (no test)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'set_global_seed' is not defined"
     ]
    }
   ],
   "source": [
    "set_global_seed(42)\n",
    "\n",
    "data_paths = [DATA_PATH / \"prp/stage_data_250hz_t1.nc\", DATA_PATH / \"prp/stage_data_250hz_t2.nc\"]\n",
    "# 80/20 train/val (no test)\n",
    "splits = split_participants_custom(data_paths, 0.15)\n",
    "\n",
    "labels_t1 = [\"negative\", \"prp_t1_1\", \"prp_t1_2\", \"prp_t1_3\"]\n",
    "labels_t2 = [\"negative\", \"prp_t2_1\", \"prp_t2_2\", \"prp_t2_3\"]\n",
    "info_to_keep = [\"participant\", \"condition\", \"trial_index\"]\n",
    "subset_cond = ('condition', 'equal', 'long')\n",
    "add_negative = True\n",
    "skip_samples = 0 # 62\n",
    "cut_samples = 0 # 63\n",
    "add_pe = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6792d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, loader, labels, participants):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    n_labels = len(labels) - 1  # Exclude 'negative' label\n",
    "    emb_dim = model.mamba_dim\n",
    "    epochs_per_participant = 1316\n",
    "\n",
    "    # Create empty dataset\n",
    "    final_ds = xr.Dataset(\n",
    "        data_vars={\n",
    "            \"embeddings\": (\n",
    "                (\"participant\", \"epochs\", \"labels\", \"emb_dim\"),\n",
    "                np.full((len(participants), epochs_per_participant, n_labels, emb_dim), np.nan, dtype=np.float32)\n",
    "            ),\n",
    "            \"condition\": ((\"participant\", \"epochs\"), np.full((len(participants), epochs_per_participant), \"\", dtype=object)),\n",
    "            # Add more info vars as needed\n",
    "        },\n",
    "        coords={\n",
    "            \"participant\": participants,\n",
    "            \"epochs\": np.arange(epochs_per_participant),\n",
    "            \"labels\": labels[1:],\n",
    "            \"emb_dim\": np.arange(emb_dim),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, total=len(loader)):\n",
    "            info = batch[2][0]\n",
    "            pred, emb = model(batch[0].to(DEVICE), return_embeddings=True)\n",
    "            pred = torch.nn.Softmax(dim=2)(pred).to(\"cpu\")\n",
    "            # pred: (batch_size, time, n_classes)\n",
    "            # emb: (batch_size, time, model_dim)\n",
    "            # info: dict of key: list\n",
    "            pred_peaks = pred[..., 1:].argmax(dim=1)\n",
    "\n",
    "            # Get embeddings for each label's peak (shape: batch × labels × emb_dim)\n",
    "            peak_emb = emb[torch.arange(pred_peaks.shape[0])[:, None], pred_peaks].to(\"cpu\")\n",
    "            \n",
    "            for i, p in enumerate(info[\"participant\"]):\n",
    "                idx = participants.index(p)\n",
    "                epoch_idx = info[\"trial_index\"][i].int()\n",
    "\n",
    "                final_ds[\"embeddings\"][idx, epoch_idx, :, :] = peak_emb[i]\n",
    "                final_ds[\"condition\"][idx, epoch_idx] = info[\"condition\"][i]\n",
    "    return final_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb36e7",
   "metadata": {},
   "source": [
    "### prp/task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363978ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_fn = norm_mad_zscore\n",
    "train_data = MultiXArrayProbaDataset(\n",
    "    [data_paths[0]],\n",
    "    participants_to_keep=splits[0],\n",
    "    normalization_fn=norm_fn,\n",
    "    labels=labels_t1,\n",
    "    info_to_keep=info_to_keep,\n",
    "    subset_cond=subset_cond,\n",
    "    add_negative=add_negative,\n",
    "    transform=Compose([StartJitterTransform(62, 1.0), EndJitterTransform(63, 1.0)]),\n",
    "    skip_samples=skip_samples,\n",
    "    cut_samples=cut_samples,\n",
    "    add_pe=add_pe,\n",
    ")\n",
    "norm_vars = get_norm_vars_from_global_statistics(train_data.statistics, norm_fn)\n",
    "class_weights = train_data.statistics[\"class_weights\"]\n",
    "val_data = MultiXArrayProbaDataset(\n",
    "    [data_paths[0]],\n",
    "    participants_to_keep=splits[1],\n",
    "    normalization_fn=norm_fn,\n",
    "    norm_vars=norm_vars,\n",
    "    labels=labels_t1,\n",
    "    info_to_keep=info_to_keep,\n",
    "    subset_cond=subset_cond,\n",
    "    add_negative=add_negative,\n",
    "    skip_samples=skip_samples,\n",
    "    cut_samples=cut_samples,\n",
    "    add_pe=add_pe,\n",
    ")\n",
    "del train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103b5359",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(\n",
    "    val_data, batch_size=32, shuffle=True, num_workers=0, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05106d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_path = Path(\"../models/t1_pe.pt\")\n",
    "checkpoint = load_model(chk_path)\n",
    "config = {\n",
    "    \"n_channels\": 64,\n",
    "    \"n_classes\": len(labels_t1),\n",
    "    \"n_mamba_layers\": 5,\n",
    "    \"use_pointconv_fe\": True,\n",
    "    \"spatial_feature_dim\": 128,\n",
    "    \"use_conv\": True,\n",
    "    \"conv_kernel_sizes\": [3, 9],\n",
    "    \"conv_in_channels\": [128, 128],\n",
    "    \"conv_out_channels\": [256, 256],\n",
    "    \"conv_concat\": True,\n",
    "    \"use_pos_enc\": add_pe,\n",
    "}\n",
    "\n",
    "model = build_mamba(config)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model = model.to(DEVICE)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33091351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7a98be947d49c1abfb32e66f246fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t1_embs_ds = get_embeddings(model, val_loader, labels_t1, splits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73f7c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_embs_ds.to_netcdf(\"files/prp_t1_embeddings.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6368343c",
   "metadata": {},
   "source": [
    "### prp/task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f284f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_fn = norm_mad_zscore\n",
    "train_data = MultiXArrayProbaDataset(\n",
    "    [data_paths[1]],\n",
    "    participants_to_keep=splits[0],\n",
    "    normalization_fn=norm_fn,\n",
    "    labels=labels_t1,\n",
    "    info_to_keep=info_to_keep,\n",
    "    subset_cond=subset_cond,\n",
    "    add_negative=add_negative,\n",
    "    transform=Compose([StartJitterTransform(62, 1.0), EndJitterTransform(63, 1.0)]),\n",
    "    skip_samples=skip_samples,\n",
    "    cut_samples=cut_samples,\n",
    "    add_pe=add_pe,\n",
    ")\n",
    "norm_vars = get_norm_vars_from_global_statistics(train_data.statistics, norm_fn)\n",
    "class_weights = train_data.statistics[\"class_weights\"]\n",
    "val_data = MultiXArrayProbaDataset(\n",
    "    [data_paths[1]],\n",
    "    participants_to_keep=splits[1],\n",
    "    normalization_fn=norm_fn,\n",
    "    norm_vars=norm_vars,\n",
    "    labels=labels_t1,\n",
    "    info_to_keep=info_to_keep,\n",
    "    subset_cond=subset_cond,\n",
    "    add_negative=add_negative,\n",
    "    skip_samples=skip_samples,\n",
    "    cut_samples=cut_samples,\n",
    "    add_pe=add_pe,\n",
    ")\n",
    "del train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fdeaf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(\n",
    "    val_data, batch_size=32, shuffle=True, num_workers=0, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_path = Path(\"../models/t2_pe.pt\")\n",
    "checkpoint = load_model(chk_path)\n",
    "config = {\n",
    "    \"n_channels\": 64,\n",
    "    \"n_classes\": len(labels_t2),\n",
    "    \"n_mamba_layers\": 5,\n",
    "    \"use_pointconv_fe\": True,\n",
    "    \"spatial_feature_dim\": 128,\n",
    "    \"use_conv\": True,\n",
    "    \"conv_kernel_sizes\": [3, 9],\n",
    "    \"conv_in_channels\": [128, 128],\n",
    "    \"conv_out_channels\": [256, 256],\n",
    "    \"conv_concat\": True,\n",
    "    \"use_pos_enc\": add_pe,\n",
    "}\n",
    "\n",
    "model = build_mamba(config)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model = model.to(DEVICE)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d74594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6fb9edb3224371b6580554f4d63349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t2_embs_ds = get_embeddings(model, val_loader, labels_t2, splits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "491b61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_embs_ds.to_netcdf(\"files/prp_t2_embeddings.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
